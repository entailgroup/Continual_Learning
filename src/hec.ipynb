{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from util_func import sMAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hec(data_root, force_reload=False):\n",
    "    try:\n",
    "        with open(data_root+\"loaded_dataset.pk\", 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "    except:\n",
    "        force_reload = True\n",
    "\n",
    "    if force_reload:\n",
    "        df = pd.DataFrame({\"MW\":[], 'hour':[], 'dayofweek':[], 'month':[], 'dayofyear':[], 'state':[]})\n",
    "        \n",
    "        for file in tqdm(os.listdir(data_root)):\n",
    "            if file[-4:] != \".csv\" or file == \"pjm_hourly_est.csv\":\n",
    "                continue\n",
    "            \n",
    "            state = re.split('_', file)[0]\n",
    "\n",
    "            temp_df = pd.read_csv('{}/{}'.format(data_root, file), parse_dates=[0])\n",
    "            temp_df['hour'] = temp_df.apply(lambda x: x['Datetime'].hour,axis=1)\n",
    "            temp_df['dayofweek'] = temp_df.apply(lambda x: x['Datetime'].dayofweek,axis=1)\n",
    "            temp_df['month'] = temp_df.apply(lambda x: x['Datetime'].month,axis=1)\n",
    "            temp_df['dayofyear'] = temp_df.apply(lambda x: x['Datetime'].dayofyear,axis=1)\n",
    "            temp_df[\"state\"] = state\n",
    "            temp_df = temp_df.rename(columns={\"{}_MW\".format(state): \"MW\"})\n",
    "            temp_df = temp_df.sort_values(\"Datetime\").drop(\"Datetime\",axis=1).reset_index(drop=True)\n",
    "            \n",
    "            df = df.append(temp_df, ignore_index=True)\n",
    "\n",
    "        with open(data_root+\"loaded_dataset.pk\", 'wb') as f:\n",
    "            pickle.dump(df, f)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_hec(df, train_prop, look_back):\n",
    "    df = df.copy()\n",
    "    train_size = int(np.ceil(df.shape[0] * train_prop))\n",
    "    sc = MinMaxScaler()\n",
    "    label_sc = MinMaxScaler()\n",
    "    \n",
    "    label_sc.fit(df[\"MW\"].values.reshape(-1,1))\n",
    "    features = [\"MW\", \"hour\", \"dayofweek\", \"month\", \"dayofyear\"]\n",
    "    df[features] = sc.fit_transform(df[features])\n",
    "    one_hot = pd.get_dummies(df[\"state\"])\n",
    "    states = df[\"state\"]\n",
    "    df.drop(\"state\", axis=1, inplace=True)\n",
    "    df = df.join(one_hot)\n",
    "    \n",
    "    data = df.to_numpy()\n",
    "\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for i in tqdm(range(look_back, len(data))):\n",
    "        if states[i-look_back:i].nunique() == 1:     \n",
    "            inputs.append(data[i-look_back:i])\n",
    "            labels.append(data[i,0])\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    labels = np.array(labels).reshape(-1,1)\n",
    "\n",
    "    X_train = inputs[:train_size]\n",
    "    y_train = labels[:train_size]\n",
    "\n",
    "    X_test = inputs[train_size:]\n",
    "    y_test = labels[train_size:]\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test), label_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae61ad2659f4c3bae8ad9d2b1afed1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60a55f4abbf4127803ce048f61df382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1090077 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jupyter-amirhossein/projects/Continual_Learning/src/hec.ipynb Cell 7'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B129.128.214.156/home/jupyter-amirhossein/projects/Continual_Learning/src/hec.ipynb#ch0000006vscode-remote?line=2'>3</a>\u001b[0m data_root \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../Datasets/Time_Series_Datasets/Hourly Energy Consumption/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B129.128.214.156/home/jupyter-amirhossein/projects/Continual_Learning/src/hec.ipynb#ch0000006vscode-remote?line=3'>4</a>\u001b[0m df \u001b[39m=\u001b[39m read_hec(data_root, force_reload\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B129.128.214.156/home/jupyter-amirhossein/projects/Continual_Learning/src/hec.ipynb#ch0000006vscode-remote?line=4'>5</a>\u001b[0m (X_train, y_train), (X_test, y_test) \u001b[39m=\u001b[39m preprocess_hec(df, train_prop, look_back)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "train_prop = 0.9\n",
    "look_back = 90\n",
    "data_root = \"../Datasets/Time_Series_Datasets/Hourly Energy Consumption/\"\n",
    "df = read_hec(data_root, force_reload=True)\n",
    "(X_train, y_train), (X_test, y_test) = preprocess_hec(df, train_prop, look_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, n_layers, n_static) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_static = n_static\n",
    "\n",
    "        self.rnn = None\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim//4)\n",
    "        self.fc2 = nn.Linear(hidden_dim//4 + n_static, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.rnn(x[:,:,:-self.n_static], h)\n",
    "        out = self.fc1(self.relu(out[:,-1]))\n",
    "        out = self.fc2(torch.cat((out, x[:,-1,-self.n_static:]), 1))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "\n",
    "class GRUNet(RNN):\n",
    "    '''GRU'''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, n_static, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__(hidden_dim, output_dim, n_layers, n_static)\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        \n",
    "\n",
    "class LSTMNet(RNN):\n",
    "    '''LSTM'''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, n_static, drop_prob=0.2):\n",
    "        super(LSTMNet, self).__init__(hidden_dim, output_dim, n_layers, n_static)\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model_type = model.__doc__ \n",
    "    model_device = 'cuda' if next(model.parameters()).is_cuda else 'cpu'\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    h = model.init_hidden(batch_size)\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for x, y in train_loader:\n",
    "        counter += 1\n",
    "        if model_type == \"GRU\":\n",
    "            h = h.data\n",
    "        else:\n",
    "            h = tuple([e.data for e in h])\n",
    "        model.zero_grad()\n",
    "        \n",
    "        out, h = model(x.to(model_device).float(), h)\n",
    "        loss = criterion(out, y.to(model_device).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), total_loss/counter))\n",
    "    current_time = time.process_time()\n",
    "    print(\"Epoch {}/{} Done, Average Loss: {}\".format(epoch, epochs, total_loss/len(train_loader)))\n",
    "    print(\"Time Elapsed for Epoch: {} seconds\".format(str(current_time-start_time)))\n",
    "        \n",
    "def evaluate(model, test_loader, criterion, label_sc):\n",
    "    model_type = model.__doc__\n",
    "    num_batches = len(test_loader)\n",
    "    model_device = 'cuda' if next(model.parameters()).is_cuda else 'cpu'\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    h = model.init_hidden(batch_size)\n",
    "    predicted_values, targets = np.empty((0,1)), np.empty((0,1))\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            if model_type == \"GRU\":\n",
    "                h = h.data\n",
    "            else:\n",
    "              h = tuple([e.data for e in h])\n",
    "\n",
    "            out, h = model(X.float().to(model_device), h)\n",
    "            test_loss += criterion(out, y.to(model_device)).item()\n",
    "            predicted_values = np.concatenate((predicted_values, out.cpu().detach().numpy().reshape(-1,1)))\n",
    "            targets = np.concatenate((targets, y.numpy()))\n",
    "        \n",
    "    test_loss /= num_batches\n",
    "    predicted_values = label_sc.inverse_transform(predicted_values)\n",
    "    targets = label_sc.inverse_transform(targets)\n",
    "    \n",
    "    print(f\"Test results: \\n sMAPE: {sMAPE(predicted_values, targets):>0.2f}% \\\n",
    "                          \\n Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "data_root = \"../Datasets/Time_Series_Datasets/Hourly Energy Consumption/\"\n",
    "train_prop = 0.9\n",
    "look_back = 90\n",
    "batch_size = 1024\n",
    "hidden_dim = 256\n",
    "input_dim = 5\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "n_static = 12\n",
    "epochs = 5\n",
    "lr = 0.001\n",
    "\n",
    "# Read Dataset\n",
    "df = read_hec(data_root)\n",
    "(X_train, y_train), (X_test, y_test), label_sc = preprocess_hec(df, train_prop, look_back)\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "\n",
    "# Create Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"==> Use accelerator: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"../trained_models/hec/trained_gru.model\")\n",
    "model = GRUNet(input_dim, hidden_dim, output_dim, n_layers, n_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "print(\"==> Start training ...\")\n",
    "print(\"Training of {} model\".format(model.__doc__))\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train(model, train_loader, optimizer, criterion)\n",
    "    evaluate(model, test_loader, criterion, label_sc)\n",
    "\n",
    "    # Update learning rate\n",
    "    if epoch % 5 == 0:\n",
    "        scheduler.step() \n",
    "\n",
    "print(\"Task done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../trained_models/hec/trained_gru.model')\n",
    "# torch.save(model, '../trained_models/hec/trained_lstm.model')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc5546ff5267140516859cc3c8e6ebbdcb51143debda87a527d4912ff07b95bf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('tsf': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
