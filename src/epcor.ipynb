{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from util_func import sMAPE, RMSE, MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_epcor(data_root, force_reload=False):\n",
    "    try:\n",
    "        with open(data_root+\"loaded_dataset.pk\", 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "    except:\n",
    "        force_reload = True\n",
    "    \n",
    "    if force_reload:\n",
    "        df = pd.read_csv(data_root+\"cons_data.csv\")\n",
    "        df[\"DATETIME\"] = df.apply(lambda x: datetime.strptime(str(x[\"DATE\"])+str(x[\"HOUR_ENDING\"]-1), \"%Y%m%d%H\"), axis=1)\n",
    "        df.drop(\"DATE\", axis=1, inplace=True)\n",
    "        df = df[[\"SITE_ID\", \"RATE_CLASS\", \"DATETIME\", \"IS_DAYLIGHT_SAVING\", \"CONSUMPTION_KWH\"]]\n",
    "        df.sort_values([\"SITE_ID\", \"DATETIME\"], ascending=True, inplace=True)\n",
    "\n",
    "        with open(data_root+\"loaded_dataset.pk\", 'wb') as f:\n",
    "            pickle.dump(df, f)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_epcor(df, train_prop, look_back):\n",
    "    train_size = int(np.ceil(df.shape[0] * train_prop))\n",
    "    n_static = 0\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"YEAR\"] = df.apply(lambda x: x[\"DATETIME\"].year - 2018, axis=1)\n",
    "    df[\"MONTH\"] = df.apply(lambda x: x[\"DATETIME\"].month, axis=1)\n",
    "    df[\"DAY\"] = df.apply(lambda x: x[\"DATETIME\"].day, axis=1)\n",
    "    df[\"WEEKDAY\"] = df.apply(lambda x: x[\"DATETIME\"].weekday(), axis=1)\n",
    "    df[\"HOUR\"] = df.apply(lambda x: x[\"DATETIME\"].hour, axis=1)\n",
    "    df = df[[\"SITE_ID\", \"YEAR\", \"MONTH\", \"DAY\", \"WEEKDAY\", \"HOUR\", \"IS_DAYLIGHT_SAVING\", \"CONSUMPTION_KWH\", \"RATE_CLASS\"]]\n",
    "\n",
    "    # TODO: try (0.1, 1) scale\n",
    "    sc = MinMaxScaler()\n",
    "    features = [\"MONTH\", \"DAY\", \"WEEKDAY\", \"HOUR\", \"CONSUMPTION_KWH\"]\n",
    "    df[features] = sc.fit_transform(df[features])\n",
    "    df[\"IS_DAYLIGHT_SAVING\"] = df[\"IS_DAYLIGHT_SAVING\"].astype(int)\n",
    "    # one_hot = pd.get_dummies(df[\"RATE_CLASS\"])\n",
    "    df.drop(\"RATE_CLASS\", axis=1, inplace=True)\n",
    "    # df = df.join(one_hot)\n",
    "    # n_static = one_hot.shape[1]\n",
    "\n",
    "    data = df.to_numpy()\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for i in tqdm(range(look_back, len(data))):\n",
    "        if len(np.unique(data[i-look_back:i, 0])) == 1:\n",
    "            \n",
    "            inputs.append(data[i-look_back:i,1:])\n",
    "            labels.append(data[i,-n_static-1])\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    labels = np.array(labels).reshape(-1,1)\n",
    "\n",
    "    X_train = inputs[:train_size]\n",
    "    y_train = labels[:train_size]\n",
    "\n",
    "    X_test = inputs[train_size:]\n",
    "    y_test = labels[train_size:]\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test), n_static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, n_layers, n_static) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_static = n_static\n",
    "\n",
    "        self.rnn = None\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        # self.fc1 = nn.Linear(hidden_dim, hidden_dim//4)\n",
    "        # self.fc2 = nn.Linear(hidden_dim//4 + n_static, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.rnn(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        # out, h = self.rnn(x[:,:,:-self.n_static], h)\n",
    "        # out = self.fc1(self.relu(out[:,-1]))\n",
    "        # out = self.fc2(torch.cat((self.relu(out), x[:,-1,-self.n_static:]), 1))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "\n",
    "class GRUNet(RNN):\n",
    "    '''GRU'''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, n_static, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__(hidden_dim, output_dim, n_layers, n_static)\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        \n",
    "\n",
    "class LSTMNet(RNN):\n",
    "    '''LSTM'''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, n_static, drop_prob=0.2):\n",
    "        super(LSTMNet, self).__init__(hidden_dim, output_dim, n_layers, n_static)\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model_type = model.__doc__ \n",
    "    model_device = 'cuda' if next(model.parameters()).is_cuda else 'cpu'\n",
    "    batch_size = train_loader.batch_size\n",
    "    model.train()\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    h = model.init_hidden(batch_size)\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for X, y in train_loader:\n",
    "        batch_size\n",
    "        counter += 1\n",
    "        if model_type == \"GRU\":\n",
    "            h = h.data\n",
    "        else:\n",
    "            h = tuple([e.data for e in h])\n",
    "        model.zero_grad()\n",
    "        \n",
    "        out, h = model(X.to(model_device).float(), h)\n",
    "        loss = criterion(out, y.to(model_device).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if counter%200 == 0:\n",
    "            print(\"Epoch {}......Step: {}/{}....... Average Loss for this step: {}\".format(epoch, counter, len(train_loader), total_loss/counter))\n",
    "    current_time = time.process_time()\n",
    "    print(\"Epoch {}/{} Done, Average Loss: {}\".format(epoch, epochs, total_loss/len(train_loader)))\n",
    "    print(\"Time Elapsed for Epoch: {} seconds\".format(str(current_time-start_time)))\n",
    "        \n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model_type = model.__doc__\n",
    "    num_batches = len(test_loader)\n",
    "    model_device = 'cuda' if next(model.parameters()).is_cuda else 'cpu'\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    h = model.init_hidden(test_loader.batch_size)\n",
    "    predicted_values, targets = np.empty((0,1)), np.empty((0,1))\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            if model_type == \"GRU\":\n",
    "                h = h.data\n",
    "            else:\n",
    "              h = tuple([e.data for e in h])\n",
    "\n",
    "            out, h = model(X.float().to(model_device), h)\n",
    "            test_loss += criterion(out, y.to(model_device)).item()\n",
    "            predicted_values = np.concatenate((predicted_values, out.cpu().detach().numpy().reshape(-1,1)))\n",
    "            targets = np.concatenate((targets, y.numpy()))\n",
    "        \n",
    "    test_loss /= num_batches\n",
    "    \n",
    "    print(f\"Test results: \\n sMAPE: {sMAPE(predicted_values, targets):>0.2f}% \\\n",
    "                          \\n RMSE:  {RMSE(predicted_values, targets):>0.2f} \\\n",
    "                          \\n MAE:   {MAE(predicted_values, targets):>0.2f} \\\n",
    "                          \\n Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a551ed1212934ebc87f9c3ae2c75be9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/299254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Use accelerator:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "data_root = \"../Datasets/Time_Series_Datasets/EPCOR/\"\n",
    "train_prop = 0.9\n",
    "look_back = 12\n",
    "batch_size = 256\n",
    "hidden_dim = 256\n",
    "input_dim = 7\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "epochs = 20\n",
    "lr = 0.001\n",
    "\n",
    "# Read Dataset\n",
    "df = read_epcor(data_root)\n",
    "df_100 = df[(df[\"SITE_ID\"] < 10) & (df[\"RATE_CLASS\"] == \"Residential\")]\n",
    "(X_train, y_train), (X_test, y_test), n_static = preprocess_epcor(df_100, train_prop, look_back)\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "\n",
    "# Create Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"==> Use accelerator: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"../trained_models/epcor/trained_gru.model\")\n",
    "model = GRUNet(input_dim, hidden_dim, output_dim, n_layers, n_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"../trained_models/epcor/trained_lstm.model\")\n",
    "model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers, n_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Start training ...\n",
      "Training of GRU model\n",
      "Epoch 1\n",
      "Epoch 1......Step: 200/1052....... Average Loss for this step: 0.00023898845031453675\n",
      "Epoch 1......Step: 400/1052....... Average Loss for this step: 0.0001386968839238989\n",
      "Epoch 1......Step: 600/1052....... Average Loss for this step: 0.00010835570379564767\n",
      "Epoch 1......Step: 800/1052....... Average Loss for this step: 8.87626659289964e-05\n",
      "Epoch 1......Step: 1000/1052....... Average Loss for this step: 7.231415008652676e-05\n",
      "Epoch 1/20 Done, Average Loss: 7.005318364147825e-05\n",
      "Time Elapsed for Epoch: 6.634225850000007 seconds\n",
      "Test results: \n",
      " sMAPE: 65.80%                           \n",
      " RMSE:  0.00                           \n",
      " MAE:   0.00                           \n",
      " Avg loss: 0.000000 \n",
      "\n",
      "Epoch 2\n",
      "Epoch 2......Step: 200/1052....... Average Loss for this step: 2.977979165734723e-05\n",
      "Epoch 2......Step: 400/1052....... Average Loss for this step: 1.738652946691843e-05\n",
      "Epoch 2......Step: 600/1052....... Average Loss for this step: 1.528773735576768e-05\n",
      "Epoch 2......Step: 800/1052....... Average Loss for this step: 1.1823024814887617e-05\n",
      "Epoch 2......Step: 1000/1052....... Average Loss for this step: 1.0978238069100143e-05\n",
      "Epoch 2/20 Done, Average Loss: 1.0465635227233992e-05\n",
      "Time Elapsed for Epoch: 6.594129260999978 seconds\n",
      "Test results: \n",
      " sMAPE: 97.38%                           \n",
      " RMSE:  0.00                           \n",
      " MAE:   0.00                           \n",
      " Avg loss: 0.000001 \n",
      "\n",
      "Epoch 3\n",
      "Epoch 3......Step: 200/1052....... Average Loss for this step: 8.489181724797846e-06\n",
      "Epoch 3......Step: 400/1052....... Average Loss for this step: 6.226085411817905e-06\n",
      "Epoch 3......Step: 600/1052....... Average Loss for this step: 9.645158655449867e-06\n",
      "Epoch 3......Step: 800/1052....... Average Loss for this step: 7.691975031569597e-06\n",
      "Epoch 3......Step: 1000/1052....... Average Loss for this step: 7.2922323566047e-06\n",
      "Epoch 3/20 Done, Average Loss: 7.329656623670882e-06\n",
      "Time Elapsed for Epoch: 6.568595981999977 seconds\n",
      "Test results: \n",
      " sMAPE: 44.94%                           \n",
      " RMSE:  0.00                           \n",
      " MAE:   0.00                           \n",
      " Avg loss: 0.000000 \n",
      "\n",
      "Epoch 4\n",
      "Epoch 4......Step: 200/1052....... Average Loss for this step: 1.3402848164574266e-05\n",
      "Epoch 4......Step: 400/1052....... Average Loss for this step: 8.210640375132527e-06\n",
      "Epoch 4......Step: 600/1052....... Average Loss for this step: 6.465245612036578e-06\n",
      "Epoch 4......Step: 800/1052....... Average Loss for this step: 1.4124154549675082e-05\n",
      "Epoch 4......Step: 1000/1052....... Average Loss for this step: 1.3111113501004468e-05\n",
      "Epoch 4/20 Done, Average Loss: 1.2478719312919714e-05\n",
      "Time Elapsed for Epoch: 6.614762632999998 seconds\n",
      "Test results: \n",
      " sMAPE: 63.88%                           \n",
      " RMSE:  0.01                           \n",
      " MAE:   0.00                           \n",
      " Avg loss: 0.000037 \n",
      "\n",
      "Epoch 5\n",
      "Epoch 5......Step: 200/1052....... Average Loss for this step: 2.4341561576761706e-05\n",
      "Epoch 5......Step: 400/1052....... Average Loss for this step: 1.3972792958441005e-05\n",
      "Epoch 5......Step: 600/1052....... Average Loss for this step: 1.234948757414145e-05\n",
      "Epoch 5......Step: 800/1052....... Average Loss for this step: 9.520122439825584e-06\n",
      "Epoch 5......Step: 1000/1052....... Average Loss for this step: 1.0960291120220234e-05\n",
      "Epoch 5/20 Done, Average Loss: 1.0424872925897394e-05\n",
      "Time Elapsed for Epoch: 6.699014043000034 seconds\n",
      "Test results: \n",
      " sMAPE: 76.61%                           \n",
      " RMSE:  0.00                           \n",
      " MAE:   0.00                           \n",
      " Avg loss: 0.000017 \n",
      "\n",
      "Epoch 6\n",
      "Epoch 6......Step: 200/1052....... Average Loss for this step: 1.1108516160405202e-05\n",
      "Epoch 6......Step: 400/1052....... Average Loss for this step: 5.936888939785945e-06\n",
      "Epoch 6......Step: 600/1052....... Average Loss for this step: 6.349452029752456e-06\n",
      "Epoch 6......Step: 800/1052....... Average Loss for this step: 8.586426655163848e-06\n",
      "Epoch 6......Step: 1000/1052....... Average Loss for this step: 7.472847730014109e-06\n",
      "Epoch 6/20 Done, Average Loss: 7.6224906957813355e-06\n",
      "Time Elapsed for Epoch: 6.61375642500002 seconds\n",
      "Test results: \n",
      " sMAPE: 100.00%                           \n",
      " RMSE:  0.01                           \n",
      " MAE:   0.01                           \n",
      " Avg loss: 0.000135 \n",
      "\n",
      "Epoch 7\n",
      "Epoch 7......Step: 200/1052....... Average Loss for this step: 7.54113083345942e-06\n",
      "Epoch 7......Step: 400/1052....... Average Loss for this step: 7.298935388426742e-06\n",
      "Epoch 7......Step: 600/1052....... Average Loss for this step: 5.910852982538017e-06\n",
      "Epoch 7......Step: 800/1052....... Average Loss for this step: 8.175513838022398e-06\n",
      "Epoch 7......Step: 1000/1052....... Average Loss for this step: 1.066648181765828e-05\n",
      "Epoch 7/20 Done, Average Loss: 1.0396197117111154e-05\n",
      "Time Elapsed for Epoch: 6.565057284999966 seconds\n",
      "Test results: \n",
      " sMAPE: 100.00%                           \n",
      " RMSE:  0.01                           \n",
      " MAE:   0.01                           \n",
      " Avg loss: 0.000101 \n",
      "\n",
      "Epoch 8\n",
      "Epoch 8......Step: 200/1052....... Average Loss for this step: 2.1631767859098262e-05\n",
      "Epoch 8......Step: 400/1052....... Average Loss for this step: 1.1750917346553535e-05\n",
      "Epoch 8......Step: 600/1052....... Average Loss for this step: 8.142206004132883e-06\n",
      "Epoch 8......Step: 800/1052....... Average Loss for this step: 6.721753021672328e-06\n",
      "Epoch 8......Step: 1000/1052....... Average Loss for this step: 6.95362185825843e-06\n",
      "Epoch 8/20 Done, Average Loss: 6.613953103820044e-06\n",
      "Time Elapsed for Epoch: 6.737398722999956 seconds\n",
      "Test results: \n",
      " sMAPE: 100.00%                           \n",
      " RMSE:  0.01                           \n",
      " MAE:   0.01                           \n",
      " Avg loss: 0.000057 \n",
      "\n",
      "Epoch 9\n",
      "Epoch 9......Step: 200/1052....... Average Loss for this step: 2.0143223471080772e-05\n",
      "Epoch 9......Step: 400/1052....... Average Loss for this step: 1.1189783691687438e-05\n",
      "Epoch 9......Step: 600/1052....... Average Loss for this step: 1.0133763982344372e-05\n",
      "Epoch 9......Step: 800/1052....... Average Loss for this step: 7.805540694021395e-06\n",
      "Epoch 9......Step: 1000/1052....... Average Loss for this step: 6.477999023292824e-06\n",
      "Epoch 9/20 Done, Average Loss: 6.161521570727131e-06\n",
      "Time Elapsed for Epoch: 6.650067239000009 seconds\n",
      "Test results: \n",
      " sMAPE: 100.00%                           \n",
      " RMSE:  0.01                           \n",
      " MAE:   0.01                           \n",
      " Avg loss: 0.000039 \n",
      "\n",
      "Epoch 10\n",
      "Epoch 10......Step: 200/1052....... Average Loss for this step: 3.918787941827873e-06\n",
      "Epoch 10......Step: 400/1052....... Average Loss for this step: 1.0192547466818524e-05\n",
      "Epoch 10......Step: 600/1052....... Average Loss for this step: 7.35952892042017e-06\n",
      "Epoch 10......Step: 800/1052....... Average Loss for this step: 7.472160208350509e-06\n",
      "Epoch 10......Step: 1000/1052....... Average Loss for this step: 6.069764377244269e-06\n",
      "Epoch 10/20 Done, Average Loss: 5.974903890243023e-06\n",
      "Time Elapsed for Epoch: 6.405957974000103 seconds\n",
      "Test results: \n",
      " sMAPE: 74.97%                           \n",
      " RMSE:  0.00                           \n",
      " MAE:   0.00                           \n",
      " Avg loss: 0.000001 \n",
      "\n",
      "Epoch 11\n",
      "Epoch 11......Step: 200/1052....... Average Loss for this step: 3.1263535059622426e-06\n",
      "Epoch 11......Step: 400/1052....... Average Loss for this step: 1.7468460550773557e-06\n",
      "Epoch 11......Step: 600/1052....... Average Loss for this step: 3.782227220744024e-06\n",
      "Epoch 11......Step: 800/1052....... Average Loss for this step: 6.244243298541008e-06\n",
      "Epoch 11......Step: 1000/1052....... Average Loss for this step: 1.0372826857860672e-05\n",
      "Epoch 11/20 Done, Average Loss: 9.891213162994164e-06\n",
      "Time Elapsed for Epoch: 6.562657664999961 seconds\n",
      "Test results: \n",
      " sMAPE: 99.80%                           \n",
      " RMSE:  0.01                           \n",
      " MAE:   0.01                           \n",
      " Avg loss: 0.000026 \n",
      "\n",
      "Epoch 12\n",
      "Epoch 12......Step: 200/1052....... Average Loss for this step: 8.071449705511214e-06\n",
      "Epoch 12......Step: 400/1052....... Average Loss for this step: 5.695398523997142e-06\n",
      "Epoch 12......Step: 600/1052....... Average Loss for this step: 4.862378946093315e-06\n",
      "Epoch 12......Step: 800/1052....... Average Loss for this step: 7.188597018226161e-06\n",
      "Epoch 12......Step: 1000/1052....... Average Loss for this step: 6.3635139100028935e-06\n",
      "Epoch 12/20 Done, Average Loss: 6.071244038259367e-06\n",
      "Time Elapsed for Epoch: 6.654883019999943 seconds\n",
      "Test results: \n",
      " sMAPE: 99.80%                           \n",
      " RMSE:  0.00                           \n",
      " MAE:   0.00                           \n",
      " Avg loss: 0.000022 \n",
      "\n",
      "Epoch 13\n",
      "Epoch 13......Step: 200/1052....... Average Loss for this step: 1.1890404819538957e-05\n",
      "Epoch 13......Step: 400/1052....... Average Loss for this step: 6.527321588531976e-06\n",
      "Epoch 13......Step: 600/1052....... Average Loss for this step: 4.8233942846032106e-06\n",
      "Epoch 13......Step: 800/1052....... Average Loss for this step: 5.732507483569016e-06\n",
      "Epoch 13......Step: 1000/1052....... Average Loss for this step: 4.927636354238985e-06\n",
      "Epoch 13/20 Done, Average Loss: 5.086479381735771e-06\n",
      "Time Elapsed for Epoch: 6.131003687999964 seconds\n",
      "Test results: \n",
      " sMAPE: 100.00%                           \n",
      " RMSE:  0.00                           \n",
      " MAE:   0.00                           \n",
      " Avg loss: 0.000005 \n",
      "\n",
      "Epoch 14\n",
      "Epoch 14......Step: 200/1052....... Average Loss for this step: 2.1181885738386086e-06\n",
      "Epoch 14......Step: 400/1052....... Average Loss for this step: 7.820156122004818e-06\n",
      "Epoch 14......Step: 600/1052....... Average Loss for this step: 5.652335771297127e-06\n",
      "Epoch 14......Step: 800/1052....... Average Loss for this step: 6.23079155471995e-06\n",
      "Epoch 14......Step: 1000/1052....... Average Loss for this step: 5.310480783027316e-06\n",
      "Epoch 14/20 Done, Average Loss: 5.05170704968359e-06\n",
      "Time Elapsed for Epoch: 6.0041365199999746 seconds\n",
      "Test results: \n",
      " sMAPE: 100.00%                           \n",
      " RMSE:  0.00                           \n",
      " MAE:   0.00                           \n",
      " Avg loss: 0.000008 \n",
      "\n",
      "Epoch 15\n",
      "Epoch 15......Step: 200/1052....... Average Loss for this step: 7.92288380504047e-06\n",
      "Epoch 15......Step: 400/1052....... Average Loss for this step: 1.3703070890063174e-05\n",
      "Epoch 15......Step: 600/1052....... Average Loss for this step: 9.545281641371399e-06\n",
      "Epoch 15......Step: 800/1052....... Average Loss for this step: 7.848043999074683e-06\n",
      "Epoch 15......Step: 1000/1052....... Average Loss for this step: 6.933427506513823e-06\n",
      "Epoch 15/20 Done, Average Loss: 6.828315512292801e-06\n",
      "Time Elapsed for Epoch: 6.181549891999907 seconds\n",
      "Test results: \n",
      " sMAPE: 78.10%                           \n",
      " RMSE:  0.00                           \n",
      " MAE:   0.00                           \n",
      " Avg loss: 0.000000 \n",
      "\n",
      "Epoch 16\n",
      "Epoch 16......Step: 200/1052....... Average Loss for this step: 1.4855937292956867e-06\n",
      "Epoch 16......Step: 400/1052....... Average Loss for this step: 1.0770936158888133e-06\n",
      "Epoch 16......Step: 600/1052....... Average Loss for this step: 1.070686914429094e-06\n",
      "Epoch 16......Step: 800/1052....... Average Loss for this step: 3.953832752370267e-06\n",
      "Epoch 16......Step: 1000/1052....... Average Loss for this step: 4.83126481363172e-06\n",
      "Epoch 16/20 Done, Average Loss: 6.21020231222332e-06\n",
      "Time Elapsed for Epoch: 6.057687596000051 seconds\n",
      "Test results: \n",
      " sMAPE: 100.00%                           \n",
      " RMSE:  0.00                           \n",
      " MAE:   0.00                           \n",
      " Avg loss: 0.000005 \n",
      "\n",
      "Epoch 17\n",
      "Epoch 17......Step: 200/1052....... Average Loss for this step: 3.284721804188706e-06\n",
      "Epoch 17......Step: 400/1052....... Average Loss for this step: 2.5900571167802866e-06\n",
      "Epoch 17......Step: 600/1052....... Average Loss for this step: 2.430870879450708e-06\n",
      "Epoch 17......Step: 800/1052....... Average Loss for this step: 2.5288447353988276e-06\n",
      "Epoch 17......Step: 1000/1052....... Average Loss for this step: 6.205700005775583e-06\n",
      "Epoch 17/20 Done, Average Loss: 5.902443533062584e-06\n",
      "Time Elapsed for Epoch: 6.01107962399999 seconds\n",
      "Test results: \n",
      " sMAPE: 100.00%                           \n",
      " RMSE:  0.00                           \n",
      " MAE:   0.00                           \n",
      " Avg loss: 0.000001 \n",
      "\n",
      "Epoch 18\n",
      "Epoch 18......Step: 200/1052....... Average Loss for this step: 1.9740894437703106e-06\n",
      "Epoch 18......Step: 400/1052....... Average Loss for this step: 1.4578880342330792e-06\n",
      "Epoch 18......Step: 600/1052....... Average Loss for this step: 5.4733873038278106e-06\n",
      "Epoch 18......Step: 800/1052....... Average Loss for this step: 4.283610062616461e-06\n",
      "Epoch 18......Step: 1000/1052....... Average Loss for this step: 4.08283997991532e-06\n",
      "Epoch 18/20 Done, Average Loss: 3.885688203873322e-06\n",
      "Time Elapsed for Epoch: 6.118762920999984 seconds\n",
      "Test results: \n",
      " sMAPE: 39.60%                           \n",
      " RMSE:  0.00                           \n",
      " MAE:   0.00                           \n",
      " Avg loss: 0.000000 \n",
      "\n",
      "Epoch 19\n",
      "Epoch 19......Step: 200/1052....... Average Loss for this step: 8.294171399931827e-06\n",
      "Epoch 19......Step: 400/1052....... Average Loss for this step: 4.182233445604311e-06\n",
      "Epoch 19......Step: 600/1052....... Average Loss for this step: 3.6697202441615672e-06\n",
      "Epoch 19......Step: 800/1052....... Average Loss for this step: 3.2760036803747285e-06\n",
      "Epoch 19......Step: 1000/1052....... Average Loss for this step: 4.24166815644611e-06\n",
      "Epoch 19/20 Done, Average Loss: 4.038324812250013e-06\n",
      "Time Elapsed for Epoch: 6.145416061999981 seconds\n",
      "Test results: \n",
      " sMAPE: 36.56%                           \n",
      " RMSE:  0.00                           \n",
      " MAE:   0.00                           \n",
      " Avg loss: 0.000000 \n",
      "\n",
      "Epoch 20\n",
      "Epoch 20......Step: 200/1052....... Average Loss for this step: 2.3416451564095553e-06\n",
      "Epoch 20......Step: 400/1052....... Average Loss for this step: 1.2899852640302355e-06\n",
      "Epoch 20......Step: 600/1052....... Average Loss for this step: 4.683038774248398e-06\n",
      "Epoch 20......Step: 800/1052....... Average Loss for this step: 5.957727884688957e-06\n",
      "Epoch 20......Step: 1000/1052....... Average Loss for this step: 5.072572683996413e-06\n",
      "Epoch 20/20 Done, Average Loss: 4.8797135082297775e-06\n",
      "Time Elapsed for Epoch: 6.183190620999994 seconds\n",
      "Test results: \n",
      " sMAPE: 34.61%                           \n",
      " RMSE:  0.00                           \n",
      " MAE:   0.00                           \n",
      " Avg loss: 0.000000 \n",
      "\n",
      "Task done!\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "print(\"==> Start training ...\")\n",
    "print(\"Training of {} model\".format(model.__doc__))\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train(model, train_loader, optimizer, criterion)\n",
    "    evaluate(model, test_loader, criterion)\n",
    "\n",
    "    # Update learning rate\n",
    "    if epoch % 5 == 0:\n",
    "        scheduler.step() \n",
    "\n",
    "print(\"Task done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../trained_models/epcor/trained_gru.model')\n",
    "# torch.save(model, '../trained_models/epcor/trained_lstm.model')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc5546ff5267140516859cc3c8e6ebbdcb51143debda87a527d4912ff07b95bf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('tsf': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
